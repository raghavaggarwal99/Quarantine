{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nimport nltk\nimport gensim\nfrom gensim import corpora","execution_count":18,"outputs":[{"output_type":"stream","text":"unable to import 'smart_open.gcs', disabling that module\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\ndoc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\ndoc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\ndoc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\ndoc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n\n# compile documents\ndoc_complete = [doc1, doc2, doc3, doc4, doc5]\n\n# Lemmatization is the process of converting a word to its base form. \n# The difference between stemming and lemmatization is, \n# lemmatization considers the context and converts the word \n# to its meaningful base form, whereas stemming just removes the last \n# few characters, often leading to incorrect meanings and spelling errors.","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\nexclude = set(string.punctuation)\n\nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(doc).split() for doc in doc_complete] \n# print(doc_clean)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n\ndictionary = corpora.Dictionary(doc_clean)\nprint(dictionary)\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\nprint(doc_term_matrix)","execution_count":24,"outputs":[{"output_type":"stream","text":"Dictionary(35 unique tokens: ['bad', 'consume', 'father', 'like', 'sister']...)\n[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)], [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)], [(2, 1), (4, 1), (18, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)], [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Trainign LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\nprint(ldamodel)","execution_count":26,"outputs":[{"output_type":"stream","text":"LdaModel(num_terms=35, num_topics=3, decay=0.5, chunksize=2000)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}